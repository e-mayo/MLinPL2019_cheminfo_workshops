{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "from src.data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use a Graph Convolutional Neural Network to solve the ESOL regression task.\n",
    "\n",
    "At the beginning you need to take the molecular graph data and transform it to a valid format in order to fit the data to PyTorch geometric layers.\n",
    "\n",
    "Then, you will have to implement a \"vanilla\" graph convolutional layer, define the network module that uses this layer and train it, using predefined hyperparameters.\n",
    "\n",
    "At the end, you will have to make a random search of hyperparameters in order to obtain the best possible prediction score.\n",
    "\n",
    "As the training loss and scoring function we will use mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filted_atomtype_list_order: [6, 8, 7, 17, 16, 9, 35, 15, 53, 'Others'], \n",
      " filted_bondtype_list_order: ['6_6', '6_8', '6_7', '6_17', '6_16', '7_8', '8_15', '6_9', '15_16', '6_35', '8_16', '7_7', 'Others']\n",
      "Transfer mol to matrices\n",
      "Done.\n",
      "filted_atomtype_list_order: [6, 8, 7, 17, 9, 35, 'Others'], \n",
      " filted_bondtype_list_order: ['6_6', '6_8', '6_7', '6_17', 'Others']\n",
      "Transfer mol to matrices\n",
      "Done.\n",
      "filted_atomtype_list_order: [6, 8, 7, 17, 9, 16, 35, 'Others'], \n",
      " filted_bondtype_list_order: ['6_6', '6_8', '6_7', '6_17', 'Others']\n",
      "Transfer mol to matrices\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "path = './data/'\n",
    "target_name = 'ESOL'\n",
    "batch_size = 32\n",
    "task = 'regression'\n",
    "\n",
    "train_dataset = load_dataset(path, target_name, 'train')\n",
    "valid_dataset = load_dataset(path, target_name, 'val')\n",
    "test_dataset = load_dataset(path, target_name, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loaders for pytorch geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every molecule in our dataset is now represented by a 3-tuple which consists of:\n",
    " - Adjacency matrix $\\in \\mathbb{M}$(n_atoms, n_atoms)\n",
    " - Atom features matrix $\\in \\mathbb{M}$(n_atoms, n_features)\n",
    " - Label (ESOL value)\n",
    " \n",
    "[PyTorch Geometric requires a different form of the data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#module-torch_geometric.data). \n",
    "\n",
    "Please write the function that transforms the data into the correct format:\n",
    " - Atom features matrix $\\in \\mathbb{M}$(n_atoms, n_features) - denoted by *x*\n",
    " - Label (ESOL value) - denoted by *y*\n",
    " - Edge indices matrix $\\in \\mathbb{M}$(2, n_edges) - denoted by *edge_index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_indices(adj):\n",
    "    edges_list = []\n",
    "    for i in range(adj.shape[0]):\n",
    "        for j in range(i, adj.shape[0]):\n",
    "            if adj[i,j] == 1:\n",
    "                edges_list.append((i,j))\n",
    "    return edges_list\n",
    "\n",
    "\n",
    "def transform_molecule_pg(mol):\n",
    "    adj = mol[0]\n",
    "    afm = mol[1]\n",
    "    label = mol[2]\n",
    "        \n",
    "    x = torch.tensor(afm)\n",
    "    y = torch.tensor(label)\n",
    "    edge_index = torch.tensor(get_edge_indices(adj)).t().contiguous()\n",
    "    \n",
    "    return Data(x=x, y=y, edge_index=edge_index)\n",
    "\n",
    "\n",
    "def transform_dataset_pg(dataset):\n",
    "    dataset_pg = []\n",
    "    \n",
    "    for mol in dataset:\n",
    "        dataset_pg.append(transform_molecule_pg(mol))\n",
    "        \n",
    "    return dataset_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = transform_dataset_pg(train_dataset)\n",
    "valid_dataset = transform_dataset_pg(valid_dataset)\n",
    "test_dataset = transform_dataset_pg(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the vanilla GCN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write a vanilla graph convolutional layer that inherits from [MessagePassing layer](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.message_passing.MessagePassing).\n",
    "\n",
    "For every atom it should take all its neighbours and then:\n",
    "1. Apply a linear layer on their feature vectors\n",
    "2. Apply a ReLU nonlinearity\n",
    "3. Aggregate information by taking the mean value of the feature vectors of all neighbours.\n",
    "\n",
    "i.e. $x^{k}_{i} = \\frac{1}{|N(i)|} \\sum_{j \\in N(i)} \\text{ReLU}(Wx_{j} + b)$, where $N(i) = \\{ j\\ |\\ (i, j) \\in E \\}$\n",
    "\n",
    "Reminder: Make sure that self loops are included in the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_GC_Layer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Vanilla_GC_Layer, self).__init__(aggr='mean')  # \"Mean\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        \n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # Step 3: Add non-linearity\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Step 4: Start propagating messages.\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train GC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please define a class for the Graph Convolutional Network that uses our predefined `Vanilla_GC_Layer`. For the input graph, network should:\n",
    "1. Pass it through some number of GC layers\n",
    "2. Aggregate information from the whole molecule, by applying global mean pooling\n",
    "3. Pass the graph embedding into the linear layer that returns the predicted value\n",
    "\n",
    "In the class definition you should include the following network hyperparameters:\n",
    " - *layers_num* - number of Graph Convolutional layers\n",
    " - *model_dim* - dimensionality of the model inner representation\n",
    " - *input_dim* - dimensionality of the input atom representation\n",
    " - *output_dim*  - dimensionality of the output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvNetwork(torch.nn.Module):\n",
    "    def __init__(self, layers_num, model_dim, \n",
    "                 input_dim, output_dim):\n",
    "        super(GraphConvNetwork, self).__init__()\n",
    "        self.layers_num = layers_num\n",
    "        \n",
    "        self.conv_layers = [Vanilla_GC_Layer(in_channels=input_dim, \n",
    "                                             out_channels=model_dim)] + \\\n",
    "                            [Vanilla_GC_Layer(in_channels=model_dim, \n",
    "                                              out_channels=model_dim) \n",
    "                            for _ in range(layers_num-1)]\n",
    "        \n",
    "        self.conv_layers = torch.nn.ModuleList(self.conv_layers)\n",
    "        self.out_layer = torch.nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        for i in range(self.layers_num):\n",
    "            data.x = self.conv_layers[i](data.x, data.edge_index)\n",
    "            \n",
    "        data.x = global_mean_pool(data.x, data.batch)\n",
    "        x = self.out_layer(data.x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following functions for training and validating our network:\n",
    "\n",
    "1. Function **train** that trains the model for a given number of epochs. The function takes the *model* and *optimizer* as parameters. For every epoch step it runs the *run_epoch* function and then it calculates the MSE loss for the valid data.  \n",
    "\n",
    "2. Function **run_epoch** that trains the model for a single epoch step.  The function takes the *model*, *optimizer* and *data_loader* as parameters. It should return the train loss for the given epoch.\n",
    "\n",
    "3. Function **valid** that calculates the scoring function - mean squared error for the given dataset. The function takes the *model* and *data_loader* as parameters. It should return the calculated MSE score.\n",
    "\n",
    "With such functions definitions, parts of the code could be re-used in the next sections of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    for epoch in range(epochs_num):\n",
    "        epoch_train_loss = run_epoch(model, optimizer, train_loader)\n",
    "        epoch_valid_loss = valid(model, valid_loader)\n",
    "        print(f'Epoch: {epoch}, train loss: {epoch_train_loss}, valid loss: {epoch_valid_loss}')\n",
    "    \n",
    "    test_loss = valid(model, test_loader)\n",
    "    print(f'End of training, test loss: {test_loss}')\n",
    "\n",
    "        \n",
    "def run_epoch(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    \n",
    "    loss_all = 0\n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data).view(-1)\n",
    "        loss = F.mse_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def valid(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred_array = []\n",
    "    true_array = []\n",
    "    \n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data).view(-1)\n",
    "\n",
    "        pred_array.append(output.detach().cpu().numpy())\n",
    "        true_array.append(data.y.cpu().numpy())\n",
    "        \n",
    "    pred_array = np.concatenate(pred_array)\n",
    "    true_array = np.concatenate(true_array)\n",
    "    \n",
    "    return mean_squared_error(true_array, pred_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some default network hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "layers_num = 3\n",
    "model_dim = 32\n",
    "input_dim = train_dataset[0].x.shape[1]\n",
    "output_dim = train_dataset[0].y.shape[0]\n",
    "\n",
    "lr = 0.0001\n",
    "epochs_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and the optimizer\n",
    "model = GraphConvNetwork(\n",
    "            layers_num=layers_num, \n",
    "            model_dim=model_dim, \n",
    "            input_dim=input_dim, \n",
    "            output_dim=output_dim).to(device)\n",
    "                   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 1.0175402674600977, valid loss: 0.8668918609619141\n",
      "Epoch: 1, train loss: 1.0146582222764613, valid loss: 0.8658786416053772\n",
      "Epoch: 2, train loss: 1.0121855645909277, valid loss: 0.865025520324707\n",
      "Epoch: 3, train loss: 1.0099143188141138, valid loss: 0.8645021319389343\n",
      "Epoch: 4, train loss: 1.0077148969316165, valid loss: 0.8636859655380249\n",
      "Epoch: 5, train loss: 1.0057702837656448, valid loss: 0.8626046776771545\n",
      "Epoch: 6, train loss: 1.004155245694247, valid loss: 0.8615769743919373\n",
      "Epoch: 7, train loss: 1.0021117693040429, valid loss: 0.8602099418640137\n",
      "Epoch: 8, train loss: 1.0001901101378803, valid loss: 0.8591213822364807\n",
      "Epoch: 9, train loss: 0.9983266911855558, valid loss: 0.8576339483261108\n",
      "Epoch: 10, train loss: 0.9962621632542156, valid loss: 0.8560516834259033\n",
      "Epoch: 11, train loss: 0.9941150420520892, valid loss: 0.8541402816772461\n",
      "Epoch: 12, train loss: 0.9918689990783742, valid loss: 0.8519949316978455\n",
      "Epoch: 13, train loss: 0.9895103635914838, valid loss: 0.8498265147209167\n",
      "Epoch: 14, train loss: 0.987053717319294, valid loss: 0.8471978306770325\n",
      "Epoch: 15, train loss: 0.9843512807875674, valid loss: 0.8450058698654175\n",
      "Epoch: 16, train loss: 0.9814441917740849, valid loss: 0.8425288796424866\n",
      "Epoch: 17, train loss: 0.9785910461866142, valid loss: 0.8399176597595215\n",
      "Epoch: 18, train loss: 0.97539966598054, valid loss: 0.8374409675598145\n",
      "Epoch: 19, train loss: 0.9723747355776722, valid loss: 0.8345687985420227\n",
      "Epoch: 20, train loss: 0.9689152564547807, valid loss: 0.8318496346473694\n",
      "Epoch: 21, train loss: 0.9653247022575919, valid loss: 0.8284933567047119\n",
      "Epoch: 22, train loss: 0.9612050188618595, valid loss: 0.8254631757736206\n",
      "Epoch: 23, train loss: 0.9571351422174542, valid loss: 0.8221485614776611\n",
      "Epoch: 24, train loss: 0.9531791956223828, valid loss: 0.8181367516517639\n",
      "Epoch: 25, train loss: 0.9485887297637711, valid loss: 0.8147708177566528\n",
      "Epoch: 26, train loss: 0.9445646344425938, valid loss: 0.8107160925865173\n",
      "Epoch: 27, train loss: 0.9395435950047689, valid loss: 0.8062505722045898\n",
      "Epoch: 28, train loss: 0.9346976785067709, valid loss: 0.8020340800285339\n",
      "Epoch: 29, train loss: 0.929648065117669, valid loss: 0.7968199849128723\n",
      "Epoch: 30, train loss: 0.9248084030764595, valid loss: 0.7912461757659912\n",
      "Epoch: 31, train loss: 0.9194430362623177, valid loss: 0.786536693572998\n",
      "Epoch: 32, train loss: 0.9133177629595586, valid loss: 0.7821164131164551\n",
      "Epoch: 33, train loss: 0.9077917379709146, valid loss: 0.7766938805580139\n",
      "Epoch: 34, train loss: 0.9019612640869327, valid loss: 0.7717412710189819\n",
      "Epoch: 35, train loss: 0.8955445416486448, valid loss: 0.7653793692588806\n",
      "Epoch: 36, train loss: 0.8887883601854751, valid loss: 0.7595966458320618\n",
      "Epoch: 37, train loss: 0.881577131753486, valid loss: 0.7520904541015625\n",
      "Epoch: 38, train loss: 0.8748388870592392, valid loss: 0.7459354400634766\n",
      "Epoch: 39, train loss: 0.8678400153067054, valid loss: 0.7387922406196594\n",
      "Epoch: 40, train loss: 0.8603571164370112, valid loss: 0.7320134043693542\n",
      "Epoch: 41, train loss: 0.8533459747181235, valid loss: 0.7261794805526733\n",
      "Epoch: 42, train loss: 0.8446681284058649, valid loss: 0.7175279855728149\n",
      "Epoch: 43, train loss: 0.8374009873808884, valid loss: 0.709341824054718\n",
      "Epoch: 44, train loss: 0.8283716313326174, valid loss: 0.7026087641716003\n",
      "Epoch: 45, train loss: 0.8203152407563711, valid loss: 0.6963148713111877\n",
      "Epoch: 46, train loss: 0.8128404013333458, valid loss: 0.6895976066589355\n",
      "Epoch: 47, train loss: 0.805211813513297, valid loss: 0.6826149821281433\n",
      "Epoch: 48, train loss: 0.7971142509295512, valid loss: 0.6739794015884399\n",
      "Epoch: 49, train loss: 0.7895842672980281, valid loss: 0.6667349934577942\n",
      "Epoch: 50, train loss: 0.7830015909909673, valid loss: 0.6598338484764099\n",
      "Epoch: 51, train loss: 0.7747465797644233, valid loss: 0.6528173089027405\n",
      "Epoch: 52, train loss: 0.7662526775490154, valid loss: 0.6469945907592773\n",
      "Epoch: 53, train loss: 0.7587295691018094, valid loss: 0.6403650641441345\n",
      "Epoch: 54, train loss: 0.7510043904680899, valid loss: 0.6335547566413879\n",
      "Epoch: 55, train loss: 0.7433322498909386, valid loss: 0.627128005027771\n",
      "Epoch: 56, train loss: 0.7359557572727458, valid loss: 0.6206366419792175\n",
      "Epoch: 57, train loss: 0.7275899961888129, valid loss: 0.6148766875267029\n",
      "Epoch: 58, train loss: 0.7206519521657221, valid loss: 0.6087112426757812\n",
      "Epoch: 59, train loss: 0.7130500841431502, valid loss: 0.6011016964912415\n",
      "Epoch: 60, train loss: 0.7068313909013096, valid loss: 0.5952517986297607\n",
      "Epoch: 61, train loss: 0.697188274823377, valid loss: 0.5889383554458618\n",
      "Epoch: 62, train loss: 0.6890229025066824, valid loss: 0.5838108658790588\n",
      "Epoch: 63, train loss: 0.6811349152725711, valid loss: 0.5776370763778687\n",
      "Epoch: 64, train loss: 0.6733658121787259, valid loss: 0.571083128452301\n",
      "Epoch: 65, train loss: 0.6651237407439036, valid loss: 0.5643651485443115\n",
      "Epoch: 66, train loss: 0.6568615217166571, valid loss: 0.5601613521575928\n",
      "Epoch: 67, train loss: 0.6481456686546427, valid loss: 0.5526521801948547\n",
      "Epoch: 68, train loss: 0.6397559397632954, valid loss: 0.546298623085022\n",
      "Epoch: 69, train loss: 0.631844992383885, valid loss: 0.5414149761199951\n",
      "Epoch: 70, train loss: 0.6240060889800213, valid loss: 0.5351554155349731\n",
      "Epoch: 71, train loss: 0.6171516206470667, valid loss: 0.5298852324485779\n",
      "Epoch: 72, train loss: 0.6070541713692397, valid loss: 0.5260560512542725\n",
      "Epoch: 73, train loss: 0.6000429029739617, valid loss: 0.5205674767494202\n",
      "Epoch: 74, train loss: 0.5930521276401575, valid loss: 0.5174448490142822\n",
      "Epoch: 75, train loss: 0.584960341783955, valid loss: 0.5097163915634155\n",
      "Epoch: 76, train loss: 0.5780414011155951, valid loss: 0.5055563449859619\n",
      "Epoch: 77, train loss: 0.5713376414749416, valid loss: 0.5013704299926758\n",
      "Epoch: 78, train loss: 0.5646427887968901, valid loss: 0.49844980239868164\n",
      "Epoch: 79, train loss: 0.5583296229199666, valid loss: 0.49530306458473206\n",
      "Epoch: 80, train loss: 0.5521141051453128, valid loss: 0.49173861742019653\n",
      "Epoch: 81, train loss: 0.5477378431154196, valid loss: 0.48680272698402405\n",
      "Epoch: 82, train loss: 0.5397099797715105, valid loss: 0.4854575991630554\n",
      "Epoch: 83, train loss: 0.5330814934622686, valid loss: 0.48033469915390015\n",
      "Epoch: 84, train loss: 0.5301710907741555, valid loss: 0.47767284512519836\n",
      "Epoch: 85, train loss: 0.5254066143754845, valid loss: 0.47471481561660767\n",
      "Epoch: 86, train loss: 0.5174910928351916, valid loss: 0.4736578166484833\n",
      "Epoch: 87, train loss: 0.5135073406072519, valid loss: 0.47068068385124207\n",
      "Epoch: 88, train loss: 0.5070796647251578, valid loss: 0.4664182960987091\n",
      "Epoch: 89, train loss: 0.5026786871865689, valid loss: 0.4669201970100403\n",
      "Epoch: 90, train loss: 0.4982462669423308, valid loss: 0.463868647813797\n",
      "Epoch: 91, train loss: 0.49302073553237574, valid loss: 0.46099206805229187\n",
      "Epoch: 92, train loss: 0.48869044606278583, valid loss: 0.46042129397392273\n",
      "Epoch: 93, train loss: 0.48530182771037794, valid loss: 0.4596119225025177\n",
      "Epoch: 94, train loss: 0.4796535017220778, valid loss: 0.455538272857666\n",
      "Epoch: 95, train loss: 0.47777938895637867, valid loss: 0.4548821449279785\n",
      "Epoch: 96, train loss: 0.4723952775387436, valid loss: 0.4534396827220917\n",
      "Epoch: 97, train loss: 0.4704853815012126, valid loss: 0.45159077644348145\n",
      "Epoch: 98, train loss: 0.4655897538043443, valid loss: 0.4510895311832428\n",
      "Epoch: 99, train loss: 0.4622821188025887, valid loss: 0.4506765604019165\n",
      "End of training, test loss: 0.532567024230957\n"
     ]
    }
   ],
   "source": [
    "# Train the network!\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search to obtain better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that takes the given number of random samples \n",
    "# from a given hyperparameters distributions.\n",
    "def make_params_grid(params, max_parameter_sets, randomize=True):\n",
    "    to_list = lambda x: [x] if not isinstance(x, Iterable) else x\n",
    "    params = {k: to_list(v) for k, v in params.items()}\n",
    "    if randomize:\n",
    "        grid = shuffle(ParameterGrid(params))\n",
    "        return grid[:max_parameter_sets]\n",
    "    return ParameterGrid(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiment we will look for the best setting of the following hyperparameters:\n",
    " - learning rate\n",
    " - epochs_num\n",
    " - batch_size\n",
    " - layers_num\n",
    " - model_dim\n",
    "\n",
    "\n",
    "You should define a function that searches for the best set of hyperparameters from a predefined distribution. \n",
    "\n",
    "1. The function **train_for_params** creates and trains the model for a single hyperparameters setting. The function takes *params* as parameters. It should:\n",
    "    1. Define the data loaders (as they need the *batch_size* parameter that we just sampled).\n",
    "    2. Define the network (as it needs the *layers_num* and *model_dim* parameters that we just sampled).\n",
    "    3. Define the optimizer (as it needs the *learning_rate* parameter that we just sampled).\n",
    "    4. Train the network with a given optimizer and data loaders, for a given number of epochs (*epochs_num* parameter that we just sampled) - here you can reuse the *run_epoch* function.\n",
    "    5. Test the trained model on valid data (you can also calculate the MSE on test data there, to make your work easier).\n",
    "    6. Return the valid and test loss for the given hyperparameters setting.\n",
    "\n",
    "2. The function **grid_search** looks for the best hyperparameters setting for a given number of trials. The function takes *param_grid* and *max_parameter_sets* as arguments. For a single step, it should take the sampled hyperparameters, call the function *train_for_params* and save the validation score of the resulted model. At the end, it should take the best model selected by finding the lowest validation score and return the test score for the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_params(params):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=params['batch_size'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
    "    \n",
    "    model = GraphConvNetwork(\n",
    "                    layers_num=params['layers_num'], \n",
    "                    model_dim=params['model_dim'], \n",
    "                    input_dim=input_dim,\n",
    "                    output_dim=output_dim).to(device)\n",
    "                   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    for epoch in range(params['epochs_num']):\n",
    "        epoch_train_loss = run_epoch(model, optimizer, train_loader)\n",
    "    \n",
    "    model_valid_loss = valid(model, valid_loader)\n",
    "    model_test_loss = valid(model, test_loader)\n",
    "    return model_valid_loss, model_test_loss\n",
    "\n",
    "def grid_search(param_grid, max_parameter_sets):\n",
    "    best_params = {}\n",
    "    best_val_score = np.inf\n",
    "    best_test_score = np.inf\n",
    "    \n",
    "    param_grid = make_params_grid(param_grid, max_parameter_sets, randomize=True)\n",
    "    \n",
    "    for i, params in enumerate(param_grid):\n",
    "        model_valid_loss, model_test_loss = train_for_params(params)\n",
    "        print(f'Model: {i} trained, valid loss: {model_valid_loss}')\n",
    "        \n",
    "        if model_valid_loss < best_val_score:\n",
    "            best_params = params\n",
    "            best_val_score = model_valid_loss\n",
    "            best_test_score = model_test_loss\n",
    "            \n",
    "    print(f'Best model valid loss: {best_val_score}, test loss: {best_test_score}')\n",
    "    print(f'Best model hyperparams: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the params grid for our random search and set the maximum number of trials\n",
    "param_grid = {\n",
    "                'lr': [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001],\n",
    "                'epochs_num': [20, 50, 100, 200],\n",
    "                'batch_size': [16, 32, 64, 128],\n",
    "                'layers_num': [1, 2, 4, 6, 8],\n",
    "                'model_dim': [16, 32, 64, 128, 256, 512],\n",
    "             }\n",
    "\n",
    "max_parameter_sets = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0 trained, valid loss: 0.40312573313713074\n",
      "Model: 1 trained, valid loss: 0.46857258677482605\n",
      "Model: 2 trained, valid loss: 0.8175060153007507\n",
      "Model: 3 trained, valid loss: 0.31216463446617126\n",
      "Model: 4 trained, valid loss: 0.3382140100002289\n",
      "Model: 5 trained, valid loss: 0.8734336495399475\n",
      "Model: 6 trained, valid loss: 0.3338407874107361\n",
      "Model: 7 trained, valid loss: 0.4891279339790344\n",
      "Model: 8 trained, valid loss: 0.434187114238739\n",
      "Model: 9 trained, valid loss: 0.6572940945625305\n",
      "Model: 10 trained, valid loss: 0.533747136592865\n",
      "Model: 11 trained, valid loss: 0.33549633622169495\n",
      "Model: 12 trained, valid loss: 0.39635682106018066\n",
      "Model: 13 trained, valid loss: 0.859822154045105\n",
      "Model: 14 trained, valid loss: 0.8818231225013733\n",
      "Model: 15 trained, valid loss: 0.4685022532939911\n",
      "Model: 16 trained, valid loss: 0.88692706823349\n",
      "Model: 17 trained, valid loss: 0.421007364988327\n",
      "Model: 18 trained, valid loss: 0.6426513195037842\n",
      "Model: 19 trained, valid loss: 0.32032981514930725\n",
      "Model: 20 trained, valid loss: 0.4536181092262268\n",
      "Model: 21 trained, valid loss: 0.45887672901153564\n",
      "Model: 22 trained, valid loss: 0.8060747385025024\n",
      "Model: 23 trained, valid loss: 0.352627158164978\n",
      "Model: 24 trained, valid loss: 0.42628014087677\n",
      "Model: 25 trained, valid loss: 0.6551514863967896\n",
      "Model: 26 trained, valid loss: 0.3623395562171936\n",
      "Model: 27 trained, valid loss: 0.38167470693588257\n",
      "Model: 28 trained, valid loss: 0.4567197263240814\n",
      "Model: 29 trained, valid loss: 0.39647185802459717\n",
      "Model: 30 trained, valid loss: 0.5280505418777466\n",
      "Model: 31 trained, valid loss: 0.290118932723999\n",
      "Model: 32 trained, valid loss: 0.46057212352752686\n",
      "Model: 33 trained, valid loss: 0.6414433717727661\n",
      "Model: 34 trained, valid loss: 0.5074124336242676\n",
      "Model: 35 trained, valid loss: 0.4872060716152191\n",
      "Model: 36 trained, valid loss: 0.3308926820755005\n",
      "Model: 37 trained, valid loss: 0.7956019043922424\n",
      "Model: 38 trained, valid loss: 0.4243488013744354\n",
      "Model: 39 trained, valid loss: 0.33616846799850464\n",
      "Model: 40 trained, valid loss: 0.33949312567710876\n",
      "Model: 41 trained, valid loss: 0.38439691066741943\n",
      "Model: 42 trained, valid loss: 0.6425028443336487\n",
      "Model: 43 trained, valid loss: 0.3729819357395172\n",
      "Model: 44 trained, valid loss: 0.45236510038375854\n",
      "Model: 45 trained, valid loss: 0.337043821811676\n",
      "Model: 46 trained, valid loss: 0.28417858481407166\n",
      "Model: 47 trained, valid loss: 0.3928295969963074\n",
      "Model: 48 trained, valid loss: 0.4444897770881653\n",
      "Model: 49 trained, valid loss: 0.48359254002571106\n",
      "Best model valid loss: 0.28417858481407166, test loss: 0.18291066586971283\n",
      "Best model hyperparams: {'model_dim': 256, 'lr': 0.0005, 'layers_num': 6, 'epochs_num': 100, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "# Search for the best model!\n",
    "grid_search(param_grid=param_grid, max_parameter_sets=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test other network settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to to obtain the best possible score, you could experiment with other network settings, such as:\n",
    " - Adding dropout\n",
    " - Adding residual connections\n",
    " - Using different types of graph convolutional layer\n",
    " - Using different aggregation types\n",
    " - Adding more layer after the aggregation\n",
    " - Extending the grid, that we used for hyperparameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
